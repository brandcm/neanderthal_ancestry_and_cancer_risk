import yaml

# assign variables
python_path = "python3"

project_directory = "/wynton/group/capra/projects/neanderthal_ancestry_and_cancer_risk/"
summary_stats_directory = (f"{project_directory}data/GWAS_summary_stats/")
significant_GWAS_variants_directory = (f"{project_directory}data/significant_GWAS_variants/")
LD_reference_panel_directory = (f"{project_directory}data/LD_reference_panels/")
retrieve_significant_GWAS_variants_script = (f"{project_directory}scripts/significant_GWAS_variants/retrieve_significant_GWAS_variants.py")

liftOver_binary = "/wynton/group/capra/bin/liftOver/liftOver"
hg38_to_hg19_chain = "/wynton/group/capra/bin/liftOver/hg38ToHg19.over.chain.gz"

with open(f"{project_directory}data/metadata/cancer_GWAS_metadata.yaml") as f:
	GWAS_metadata = yaml.safe_load(f)

GWASs = list(GWAS_metadata.keys())

hg19_native_GWASs = [GWAS for GWAS in GWASs if GWAS_metadata[GWAS]['assembly'] == "hg19"]
hg38_native_GWASs = [GWAS for GWAS in GWASs if GWAS_metadata[GWAS]['assembly'] == "hg38"]

chromosomes = [str(i) for i in range(1, 23)]

ruleorder: liftOver_significant_GWAS_SNVs > retrieve_hg19_GWAS_significant_SNVs

# rules
rule all:
	input:
		f"{significant_GWAS_variants_directory}cancer_GWAS_N_SNVs_and_N_significant_SNVs.txt",
		expand(f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_hg19_r2_0.5_LD_clumped_significant_SNVs.txt.gz", GWAS=GWASs),
		expand(f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_hg19_r2_0.8_LD_clumped_significant_SNVs.txt.gz", GWAS=GWASs)

rule retrieve_hg19_GWAS_significant_SNVs:
	input:
		GWAS_file = lambda wildcards: GWAS_metadata[wildcards.GWAS]["input"]
	output:
		f"{significant_GWAS_variants_directory}{{GWAS}}_hg19_significant_SNVs.txt.gz"
	wildcard_constraints:
		GWAS = "|".join(hg19_native_GWASs)
	params:
		python=python_path,
		script=retrieve_significant_GWAS_variants_script,
		chr_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["chr_column"],
		pos_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["pos_column"],
		rsID_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["rsID_column"],
		effect_allele_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["effect_allele_column"],
		other_allele_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["other_allele_column"],
		beta_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["beta_column"],
		p_value_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["p_value_column"]
	resources:
		mem=10,
		threads=1,
		time="2:00:00"
	shell:
		"""
		{params.python} \
		{params.script} \
		--input {input.GWAS_file} \
		--assembly hg19 \
		--chr_column {params.chr_col} \
		--pos_column {params.pos_col} \
		--rsID_column {params.rsID_col} \
		--effect_allele_column {params.effect_allele_col} \
		--other_allele_column {params.other_allele_col} \
		--beta_column {params.beta_col} \
		--p_value_column {params.p_value_col} \
		--output {output}
		"""

rule retrieve_hg38_GWAS_significant_SNVs:
	input:
		GWAS_file = lambda wildcards: GWAS_metadata[wildcards.GWAS]["input"]
	output:
		f"{significant_GWAS_variants_directory}{{GWAS}}_hg38_significant_SNVs.txt.gz"
	params:
		python=python_path,
		script=retrieve_significant_GWAS_variants_script,
		chr_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["chr_column"],
		pos_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["pos_column"],
		rsID_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["rsID_column"],
		effect_allele_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["effect_allele_column"],
		other_allele_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["other_allele_column"],
		beta_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["beta_column"],
		p_value_col=lambda wildcards: GWAS_metadata[wildcards.GWAS]["p_value_column"]
	resources:
		mem=10,
		threads=1,
		time="2:00:00"
	shell:
		"""
		{params.python} \
		{params.script} \
		--input {input.GWAS_file} \
		--assembly hg38 \
		--chr_column {params.chr_col} \
		--pos_column {params.pos_col} \
		--rsID_column {params.rsID_col} \
		--effect_allele_column {params.effect_allele_col} \
		--other_allele_column {params.other_allele_col} \
		--beta_column {params.beta_col} \
		--p_value_column {params.p_value_col} \
		--output {output}
		"""

rule calculate_hg19_GWAS_stats:
	input:
		GWAS_file = lambda wildcards: GWAS_metadata[wildcards.GWAS]["input"],
		significant_variants_file = f"{significant_GWAS_variants_directory}{{GWAS}}_hg19_significant_SNVs.txt.gz"
	output:
		temp_stats = temp(f"{significant_GWAS_variants_directory}{{GWAS}}_stats_hg19.tmp")
	resources:
		mem=10,
		threads=1,
		time="2:00:00"
	run:
		import pandas as pd

		meta = GWAS_metadata[wildcards.GWAS]
		df = pd.read_csv(input.GWAS_file, sep='\t', usecols=[meta["chr_column"], meta["pos_column"], meta["effect_allele_column"], meta["other_allele_column"], meta["rsID_column"], meta["beta_column"], meta["p_value_column"]])
		
		filtered_SNVs = df[
			(df[meta["effect_allele_column"]].astype(str).str.len() == 1) &
			(df[meta["other_allele_column"]].astype(str).str.len() == 1)
		]

		significant_variants_df = pd.read_csv(input.significant_variants_file, sep='\t', compression='gzip')
		
		pd.DataFrame([{'GWAS_file': wildcards.GWAS, 'N_GWAS_SNVs': len(filtered_SNVs), 'N_significant_GWAS_SNVs': len(significant_variants_df)}]).to_csv(output.temp_stats, sep='\t', index=False)

rule calculate_hg38_GWAS_stats:
	input:
		GWAS_file = lambda wildcards: GWAS_metadata[wildcards.GWAS]["input"],
		significant_variants_file = f"{significant_GWAS_variants_directory}{{GWAS}}_hg38_significant_SNVs.txt.gz"
	output:
		temp_stats = temp(f"{significant_GWAS_variants_directory}{{GWAS}}_stats_hg38.tmp")
	resources:
		mem=10,
		threads=1,
		time="2:00:00"
	run:
		import pandas as pd

		meta = GWAS_metadata[wildcards.GWAS]
		df = pd.read_csv(input.GWAS_file, sep='\t', usecols=[meta["chr_column"], meta["pos_column"], meta["effect_allele_column"], meta["other_allele_column"], meta["rsID_column"], meta["beta_column"], meta["p_value_column"]])
		
		filtered_SNVs = df[
			(df[meta["effect_allele_column"]].astype(str).str.len() == 1) &
			(df[meta["other_allele_column"]].astype(str).str.len() == 1)
		]

		significant_variants_df = pd.read_csv(input.significant_variants_file, sep='\t', compression='gzip')
		
		pd.DataFrame([{'GWAS_file': wildcards.GWAS, 'N_GWAS_SNVs': len(filtered_SNVs), 'N_significant_GWAS_SNVs': len(significant_variants_df)}]).to_csv(output.temp_stats, sep='\t', index=False)

rule concat_GWAS_stats:
	input:
		all_stats = expand(f"{significant_GWAS_variants_directory}{{GWAS}}_stats_hg19.tmp", GWAS=hg19_native_GWASs) + \
					expand(f"{significant_GWAS_variants_directory}{{GWAS}}_stats_hg38.tmp", GWAS=hg38_native_GWASs)
	output:
		summary = f"{significant_GWAS_variants_directory}cancer_GWAS_N_SNVs_and_N_significant_SNVs.txt"
	resources:
		mem=10,
		threads=1,
		time="2:00:00"
	run:
		import pandas as pd

		combined_df = pd.concat([pd.read_csv(f, sep='\t') for f in input.all_stats])
		combined_df.sort_values('GWAS_file', inplace=True)
		combined_df.to_csv(output.summary, sep='\t', index=False)

rule liftOver_significant_GWAS_SNVs:
	input:
		f"{significant_GWAS_variants_directory}{{GWAS}}_hg38_significant_SNVs.txt.gz"
	output:
		lifted=f"{significant_GWAS_variants_directory}{{GWAS}}_hg19_significant_SNVs.txt.gz",
		unlifted=f"{significant_GWAS_variants_directory}unlifted/{{GWAS}}_hg38.unlifted"
	wildcard_constraints:
		GWAS = "|".join(hg38_native_GWASs)
	params:
		liftOver=liftOver_binary,
		chain=hg38_to_hg19_chain
	resources:
		mem=5,
		threads=1,
		time="1:00:00"
	shell:
		"""
		hg38_tmp="{output.lifted}_hg38.tmp"
		hg19_tmp="{output.lifted}_hg19.tmp"

		zcat {input} | tail -n +2 | \
		awk -v OFS="\t" '{{print "chr"$1, $2-1, $2, $3";"$4";"$5";"$6";"$7}}' \
		> $hg38_tmp

		{params.liftOver} $hg38_tmp {params.chain} $hg19_tmp {output.unlifted}

		{{
			echo -e "chr\tpos\trsID\teffect_allele\tother_allele\tbeta\tp_value"
			awk -F'\t|;' -v OFS="\t" '{{gsub(/^chr/, "", $1); print $1, $3, $4, $5, $6, $7, $8}}' $hg19_tmp
		}} | gzip > {output.lifted}

		# Clean up job-specific temp files
		rm $hg38_tmp $hg19_tmp
		"""

rule LD_clump_significant_GWAS_SNVs:
	input:
		GWAS_file=f"{significant_GWAS_variants_directory}{{GWAS}}_hg19_significant_SNVs.txt.gz",
		reference_bed=lambda wildcards: (
			f"{LD_reference_panel_directory}{'EAS' if '_EAS_' in wildcards.GWAS else 'EUR'}/1000G_{'EAS' if '_EAS_' in wildcards.GWAS else 'EUR'}_chr{wildcards.chr}_nodups.bed"
		)
	output:
		r2_5=temp(f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_chr{{chr}}_r2_0.5_LD_clumped_significant_SNVs.clumped"),
		r2_8=temp(f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_chr{{chr}}_r2_0.8_LD_clumped_significant_SNVs.clumped")
	params:
		reference_prefix=lambda wildcards, input: input.reference_bed.replace(".bed", ""),
		out_prefix_5=f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_chr{{chr}}_r2_0.5_LD_clumped_significant_SNVs",
		out_prefix_8=f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_chr{{chr}}_r2_0.8_LD_clumped_significant_SNVs"
	resources:
		mem=10,
		threads=1,
		time="2:00:00"
	shell:
		"""
		line_count=$(zcat {input.GWAS_file} | wc -l)

		if [ "$line_count" -gt 1 ]; then
			plink --bfile {params.reference_prefix} --clump {input.GWAS_file} \
			--clump-snp-field rsID --clump-field p_value --clump-p1 1 --clump-p2 1 \
			--clump-r2 0.5 --clump-kb 250 --out {params.out_prefix_5} || true
		fi

		if [ ! -f "{output.r2_5}" ]; then
			echo -e " CHR\tF\tSNP\tBP\tP\tTOTAL\tNSIG\tS05\tS01\tS001\tS0001\tSP2" > {output.r2_5}
		fi

		if [ "$line_count" -gt 1 ]; then
			plink --bfile {params.reference_prefix} --clump {input.GWAS_file} \
			--clump-snp-field rsID --clump-field p_value --clump-p1 1 --clump-p2 1 \
			--clump-r2 0.8 --clump-kb 250 --out {params.out_prefix_8} || true
		fi

		if [ ! -f "{output.r2_8}" ]; then
			echo -e " CHR\tF\tSNP\tBP\tP\tTOTAL\tNSIG\tS05\tS01\tS001\tS0001\tSP2" > {output.r2_8}
		fi
		"""

rule concat_LD_clumped_significant_GWAS_SNVs:
	input:
		r2_5 = expand(f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_chr{{chr}}_r2_0.5_LD_clumped_significant_SNVs.clumped", GWAS="{GWAS}", chr=chromosomes),
		r2_8 = expand(f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_chr{{chr}}_r2_0.8_LD_clumped_significant_SNVs.clumped", GWAS="{GWAS}", chr=chromosomes)
	output:
		r2_5 = f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_hg19_r2_0.5_LD_clumped_significant_SNVs.txt.gz",
		r2_8 = f"{significant_GWAS_variants_directory}LD_clumped_significant_SNVs/{{GWAS}}_hg19_r2_0.8_LD_clumped_significant_SNVs.txt.gz"
	resources:
		mem=10,
		threads=1,
		time="2:00:00"
	run:
		import gzip
		import os
		import pandas as pd

		for r2 in ["r2_5", "r2_8"]:
			input_files = input[r2]
			out_file = output[r2]

			dfs = []
			for f in input_files:
				if os.path.exists(f) and os.path.getsize(f) > 0:
					try:
						df = pd.read_csv(f, delim_whitespace=True)
						if not df.empty:
							dfs.append(df)
					except pd.errors.EmptyDataError:
						continue

			if dfs:
				combined_df = pd.concat(dfs, ignore_index=True)

				combined_df['CHR_int'] = pd.to_numeric(combined_df['CHR'], errors='coerce')
				combined_df.sort_values(['CHR_int', 'BP'], inplace=True)
				combined_df.drop(columns=['CHR_int'], inplace=True)

				combined_df.to_csv(out_file, sep='\t', index=False, compression='gzip')
			else:
				with gzip.open(out_file, 'wt') as f:
					f.write("CHR\tSNP\tBP\tP\tTOTAL\tNSIG\tS05\tS01\tS001\tS0001\tSP2\n")